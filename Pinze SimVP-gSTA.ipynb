{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from typing import Optional as _Optional\n",
    "import numpy as np\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tesla V100-PCIE-32GB'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DWConv(nn.Module):\n",
    "    def __init__(self, dim=768):\n",
    "        super(DWConv, self).__init__()\n",
    "        self.dwconv = nn.Conv2d(dim, dim, 3, 1, 1, bias=True, groups=dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dwconv(x)\n",
    "        return x\n",
    "\n",
    "def _no_grad_trunc_normal_(tensor, mean, std, a, b, generator=None):\n",
    "    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n",
    "    def norm_cdf(x):\n",
    "        # Computes standard normal cumulative distribution function\n",
    "        return (1. + math.erf(x / math.sqrt(2.))) / 2.\n",
    "\n",
    "    if (mean < a - 2 * std) or (mean > b + 2 * std):\n",
    "        warnings.warn(\"mean is more than 2 std from [a, b] in nn.init.trunc_normal_. \"\n",
    "                      \"The distribution of values may be incorrect.\",\n",
    "                      stacklevel=2)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Values are generated by using a truncated uniform distribution and\n",
    "        # then using the inverse CDF for the normal distribution.\n",
    "        # Get upper and lower cdf values\n",
    "        l = norm_cdf((a - mean) / std)\n",
    "        u = norm_cdf((b - mean) / std)\n",
    "\n",
    "        # Uniformly fill tensor with values from [l, u], then translate to\n",
    "        # [2l-1, 2u-1].\n",
    "        tensor.uniform_(2 * l - 1, 2 * u - 1, generator=generator)\n",
    "\n",
    "        # Use inverse cdf transform for normal distribution to get truncated\n",
    "        # standard normal\n",
    "        tensor.erfinv_()\n",
    "\n",
    "        # Transform to proper mean, std\n",
    "        tensor.mul_(std * math.sqrt(2.))\n",
    "        tensor.add_(mean)\n",
    "\n",
    "        # Clamp to ensure it's in the proper range\n",
    "        tensor.clamp_(min=a, max=b)\n",
    "        return tensor\n",
    "    \n",
    "def trunc_normal_(\n",
    "    tensor: Tensor,\n",
    "    mean: float = 0.,\n",
    "    std: float = 1.,\n",
    "    a: float = -2.,\n",
    "    b: float = 2.,\n",
    "    generator: _Optional[torch.Generator] = None\n",
    ") -> Tensor:\n",
    "\n",
    "    return _no_grad_trunc_normal_(tensor, mean, std, a, b, generator=generator)\n",
    "\n",
    "def drop_path(x, drop_prob: float = 0., training: bool = False, scale_by_keep: bool = True):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n",
    "\n",
    "    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,\n",
    "    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n",
    "    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for\n",
    "    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use\n",
    "    'survival rate' as the argument.\n",
    "\n",
    "    \"\"\"\n",
    "    if drop_prob == 0. or not training:\n",
    "        return x\n",
    "    keep_prob = 1 - drop_prob\n",
    "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
    "    random_tensor = x.new_empty(shape).bernoulli_(keep_prob)\n",
    "    if keep_prob > 0.0 and scale_by_keep:\n",
    "        random_tensor.div_(keep_prob)\n",
    "    return x * random_tensor\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
    "    \"\"\"\n",
    "    def __init__(self, drop_prob: float = 0., scale_by_keep: bool = True):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.scale_by_keep = scale_by_keep\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training, self.scale_by_keep)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return f'drop_prob={round(self.drop_prob,3):0.3f}'\n",
    "\n",
    "    \n",
    "class MixMlp(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Conv2d(in_features, hidden_features, 1)  # 1x1\n",
    "        self.dwconv = DWConv(hidden_features)                  # CFF: Convlutional feed-forward network\n",
    "        self.act = act_layer()                                 # GELU\n",
    "        self.fc2 = nn.Conv2d(hidden_features, out_features, 1) # 1x1\n",
    "        self.drop = nn.Dropout(drop)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "            fan_out //= m.groups\n",
    "            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.dwconv(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "class BasicConv2d(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 kernel_size=3,\n",
    "                 stride=1,\n",
    "                 padding=0,\n",
    "                 dilation=1,\n",
    "                 upsampling=False,\n",
    "                 act_norm=False,\n",
    "                 act_inplace=True):\n",
    "        super(BasicConv2d, self).__init__()\n",
    "        self.act_norm = act_norm\n",
    "        if upsampling is True:\n",
    "            self.conv = nn.Sequential(*[\n",
    "                nn.Conv2d(in_channels, out_channels*4, kernel_size=kernel_size,\n",
    "                          stride=1, padding=padding, dilation=dilation),\n",
    "                nn.PixelShuffle(2)\n",
    "            ])\n",
    "        else:\n",
    "            self.conv = nn.Conv2d(\n",
    "                in_channels, out_channels, kernel_size=kernel_size,\n",
    "                stride=stride, padding=padding, dilation=dilation)\n",
    "\n",
    "        self.norm = nn.GroupNorm(2, out_channels)\n",
    "        self.act = nn.SiLU(inplace=act_inplace)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, (nn.Conv2d)):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.conv(x)\n",
    "        if self.act_norm:\n",
    "            y = self.act(self.norm(y))\n",
    "        return y\n",
    "\n",
    "\n",
    "class ConvSC(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 C_in,\n",
    "                 C_out,\n",
    "                 kernel_size=3,\n",
    "                 downsampling=False,\n",
    "                 upsampling=False,\n",
    "                 act_norm=True,\n",
    "                 act_inplace=True):\n",
    "        super(ConvSC, self).__init__()\n",
    "\n",
    "        stride = 2 if downsampling is True else 1\n",
    "        padding = (kernel_size - stride + 1) // 2\n",
    "\n",
    "        self.conv = BasicConv2d(C_in, C_out, kernel_size=kernel_size, stride=stride,\n",
    "                                upsampling=upsampling, padding=padding,\n",
    "                                act_norm=act_norm, act_inplace=act_inplace)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.conv(x)\n",
    "        return y\n",
    "\n",
    "\n",
    "class GroupConv2d(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 kernel_size=3,\n",
    "                 stride=1,\n",
    "                 padding=0,\n",
    "                 groups=1,\n",
    "                 act_norm=False,\n",
    "                 act_inplace=True):\n",
    "        super(GroupConv2d, self).__init__()\n",
    "        self.act_norm=act_norm\n",
    "        if in_channels % groups != 0:\n",
    "            groups=1\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels, out_channels, kernel_size=kernel_size,\n",
    "            stride=stride, padding=padding, groups=groups)\n",
    "        self.norm = nn.GroupNorm(groups,out_channels)\n",
    "        self.activate = nn.LeakyReLU(0.2, inplace=act_inplace)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.conv(x)\n",
    "        if self.act_norm:\n",
    "            y = self.activate(self.norm(y))\n",
    "        return y\n",
    "\n",
    "\n",
    "class gInception_ST(nn.Module):\n",
    "    \"\"\"A IncepU block for SimVP\"\"\"\n",
    "\n",
    "    def __init__(self, C_in, C_hid, C_out, incep_ker = [3,5,7,11], groups = 8):        \n",
    "        super(gInception_ST, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(C_in, C_hid, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "        layers = []\n",
    "        for ker in incep_ker:\n",
    "            layers.append(GroupConv2d(\n",
    "                C_hid, C_out, kernel_size=ker, stride=1,\n",
    "                padding=ker//2, groups=groups, act_norm=True))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        y = 0\n",
    "        for layer in self.layers:\n",
    "            y += layer(x)\n",
    "        return y\n",
    "\n",
    "\n",
    "class AttentionModule(nn.Module):\n",
    "    \"\"\"Large Kernel Attention for SimVP\"\"\"\n",
    "\n",
    "    def __init__(self, dim, kernel_size, dilation=3):\n",
    "        super().__init__()\n",
    "        d_k = 2 * dilation - 1\n",
    "        d_p = (d_k - 1) // 2\n",
    "        dd_k = kernel_size // dilation + ((kernel_size // dilation) % 2 - 1)\n",
    "        dd_p = (dilation * (dd_k - 1) // 2)\n",
    "\n",
    "        self.conv0 = nn.Conv2d(dim, dim, d_k, padding=d_p, groups=dim)\n",
    "        self.conv_spatial = nn.Conv2d(\n",
    "            dim, dim, dd_k, stride=1, padding=dd_p, groups=dim, dilation=dilation)\n",
    "        self.conv1 = nn.Conv2d(dim, 2*dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        u = x.clone()\n",
    "        attn = self.conv0(x)           # depth-wise conv\n",
    "        attn = self.conv_spatial(attn) # depth-wise dilation convolution\n",
    "        \n",
    "        f_g = self.conv1(attn)\n",
    "        split_dim = f_g.shape[1] // 2\n",
    "        f_x, g_x = torch.split(f_g, split_dim, dim=1)\n",
    "        return torch.sigmoid(g_x) * f_x\n",
    "\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    \"\"\"A Spatial Attention block for SimVP\"\"\"\n",
    "\n",
    "    def __init__(self, d_model, kernel_size=21, attn_shortcut=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.proj_1 = nn.Conv2d(d_model, d_model, 1)         # 1x1 conv\n",
    "        self.activation = nn.GELU()                          # GELU\n",
    "        self.spatial_gating_unit = AttentionModule(d_model, kernel_size)\n",
    "        self.proj_2 = nn.Conv2d(d_model, d_model, 1)         # 1x1 conv\n",
    "        self.attn_shortcut = attn_shortcut\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.attn_shortcut:\n",
    "            shortcut = x.clone()\n",
    "        x = self.proj_1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.spatial_gating_unit(x)\n",
    "        x = self.proj_2(x)\n",
    "        if self.attn_shortcut:\n",
    "            x = x + shortcut\n",
    "        return x\n",
    "\n",
    "\n",
    "class GASubBlock(nn.Module):\n",
    "    \"\"\"A GABlock (gSTA) for SimVP\"\"\"\n",
    "\n",
    "    def __init__(self, dim, kernel_size=21, mlp_ratio=4.,\n",
    "                 drop=0., drop_path=0.1, init_value=1e-2, act_layer=nn.GELU):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.BatchNorm2d(dim)\n",
    "        self.attn = SpatialAttention(dim, kernel_size)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "        self.norm2 = nn.BatchNorm2d(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = MixMlp(\n",
    "            in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "        self.layer_scale_1 = nn.Parameter(init_value * torch.ones((dim)), requires_grad=True)\n",
    "        self.layer_scale_2 = nn.Parameter(init_value * torch.ones((dim)), requires_grad=True)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "            fan_out //= m.groups\n",
    "            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'layer_scale_1', 'layer_scale_2'}\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.drop_path(\n",
    "            self.layer_scale_1.unsqueeze(-1).unsqueeze(-1) * self.attn(self.norm1(x)))\n",
    "        x = x + self.drop_path(\n",
    "            self.layer_scale_2.unsqueeze(-1).unsqueeze(-1) * self.mlp(self.norm2(x)))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimVP_Model(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self, in_shape, hid_S=16, hid_T=256, N_S=4, N_T=4, model_type='gSTA',\n",
    "                 mlp_ratio=8., drop=0.0, drop_path=0.0, spatio_kernel_enc=3,\n",
    "                 spatio_kernel_dec=3, act_inplace=True, **kwargs):\n",
    "        super(SimVP_Model, self).__init__()\n",
    "        T, C, H, W = in_shape  # T is pre_seq_length\n",
    "        H, W = int(H / 2**(N_S/2)), int(W / 2**(N_S/2))  # downsample 1 / 2**(N_S/2)\n",
    "        act_inplace = False\n",
    "        self.enc = Encoder(C, hid_S, N_S, spatio_kernel_enc, act_inplace=act_inplace)\n",
    "        self.dec = Decoder(hid_S, C, N_S, spatio_kernel_dec, act_inplace=act_inplace)\n",
    "\n",
    "        model_type = 'gsta' if model_type is None else model_type.lower()\n",
    "        if model_type == 'incepu':\n",
    "            self.hid = MidIncepNet(T*hid_S, hid_T, N_T)\n",
    "        else:\n",
    "            self.hid = MidMetaNet(T*hid_S, hid_T, N_T,\n",
    "                input_resolution=(H, W), model_type=model_type,\n",
    "                mlp_ratio=mlp_ratio, drop=drop, drop_path=drop_path)\n",
    "\n",
    "    def forward(self, x_raw, **kwargs):\n",
    "        B, T, C, H, W = x_raw.shape\n",
    "        x = x_raw.reshape(B*T, C, H, W)\n",
    "\n",
    "        embed, skip = self.enc(x)\n",
    "        _, C_, H_, W_ = embed.shape\n",
    "\n",
    "        z = embed.view(B, T, C_, H_, W_)\n",
    "        hid = self.hid(z)\n",
    "        hid = hid.reshape(B*T, C_, H_, W_)\n",
    "\n",
    "        Y = self.dec(hid, skip)\n",
    "        Y = Y.reshape(B, T, C, H, W)\n",
    "\n",
    "        return Y\n",
    "\n",
    "\n",
    "def sampling_generator(N, reverse=False):\n",
    "    samplings = [False, True] * (N // 2)\n",
    "    if reverse: return list(reversed(samplings[:N]))\n",
    "    else: return samplings[:N]\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"3D Encoder for SimVP\"\"\"\n",
    "\n",
    "    def __init__(self, C_in, C_hid, N_S, spatio_kernel, act_inplace=True):\n",
    "        samplings = sampling_generator(N_S)\n",
    "        super(Encoder, self).__init__()\n",
    "        self.enc = nn.Sequential(\n",
    "              ConvSC(C_in, C_hid, spatio_kernel, downsampling=samplings[0],\n",
    "                     act_inplace=act_inplace),\n",
    "            *[ConvSC(C_hid, C_hid, spatio_kernel, downsampling=s,\n",
    "                     act_inplace=act_inplace) for s in samplings[1:]]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):  # B*4, 3, 128, 128\n",
    "        enc1 = self.enc[0](x)\n",
    "        latent = enc1\n",
    "        for i in range(1, len(self.enc)):\n",
    "            latent = self.enc[i](latent)\n",
    "        return latent, enc1\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"3D Decoder for SimVP\"\"\"\n",
    "\n",
    "    def __init__(self, C_hid, C_out, N_S, spatio_kernel, act_inplace=True):\n",
    "        samplings = sampling_generator(N_S, reverse=True)\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dec = nn.Sequential(\n",
    "            *[ConvSC(C_hid, C_hid, spatio_kernel, upsampling=s,\n",
    "                     act_inplace=act_inplace) for s in samplings[:-1]],\n",
    "              ConvSC(C_hid, C_hid, spatio_kernel, upsampling=samplings[-1],\n",
    "                     act_inplace=act_inplace)\n",
    "        )\n",
    "        self.readout = nn.Conv2d(C_hid, C_out, 1)\n",
    "\n",
    "    def forward(self, hid, enc1=None):\n",
    "        for i in range(0, len(self.dec)-1):\n",
    "            hid = self.dec[i](hid)\n",
    "        Y = self.dec[-1](hid + enc1)\n",
    "        Y = self.readout(Y)\n",
    "        return Y\n",
    "\n",
    "\n",
    "class MidIncepNet(nn.Module):\n",
    "    \"\"\"The hidden Translator of IncepNet for SimVPv1\"\"\"\n",
    "\n",
    "    def __init__(self, channel_in, channel_hid, N2, incep_ker=[3,5,7,11], groups=8, **kwargs):\n",
    "        super(MidIncepNet, self).__init__()\n",
    "        assert N2 >= 2 and len(incep_ker) > 1\n",
    "        self.N2 = N2\n",
    "        enc_layers = [gInception_ST(\n",
    "            channel_in, channel_hid//2, channel_hid, incep_ker= incep_ker, groups=groups)]\n",
    "        for i in range(1,N2-1):\n",
    "            enc_layers.append(\n",
    "                gInception_ST(channel_hid, channel_hid//2, channel_hid,\n",
    "                              incep_ker=incep_ker, groups=groups))\n",
    "        enc_layers.append(\n",
    "                gInception_ST(channel_hid, channel_hid//2, channel_hid,\n",
    "                              incep_ker=incep_ker, groups=groups))\n",
    "        dec_layers = [\n",
    "                gInception_ST(channel_hid, channel_hid//2, channel_hid,\n",
    "                              incep_ker=incep_ker, groups=groups)]\n",
    "        for i in range(1,N2-1):\n",
    "            dec_layers.append(\n",
    "                gInception_ST(2*channel_hid, channel_hid//2, channel_hid,\n",
    "                              incep_ker=incep_ker, groups=groups))\n",
    "        dec_layers.append(\n",
    "                gInception_ST(2*channel_hid, channel_hid//2, channel_in,\n",
    "                              incep_ker=incep_ker, groups=groups))\n",
    "\n",
    "        self.enc = nn.Sequential(*enc_layers)\n",
    "        self.dec = nn.Sequential(*dec_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C, H, W = x.shape\n",
    "        x = x.reshape(B, T*C, H, W)\n",
    "\n",
    "        # encoder\n",
    "        skips = []\n",
    "        z = x\n",
    "        for i in range(self.N2):\n",
    "            z = self.enc[i](z)\n",
    "            if i < self.N2-1:\n",
    "                skips.append(z)\n",
    "        # decoder\n",
    "        z = self.dec[0](z)\n",
    "        for i in range(1,self.N2):\n",
    "            z = self.dec[i](torch.cat([z, skips[-i]], dim=1) )\n",
    "\n",
    "        y = z.reshape(B, T, C, H, W)\n",
    "        return y\n",
    "\n",
    "\n",
    "class MetaBlock(nn.Module):\n",
    "    \"\"\"The hidden Translator of MetaFormer for SimVP\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, input_resolution=None, model_type=None,\n",
    "                 mlp_ratio=8., drop=0.0, drop_path=0.0, layer_i=0):\n",
    "        super(MetaBlock, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        model_type = model_type.lower() if model_type is not None else 'gsta'\n",
    "\n",
    "        if model_type == 'gsta':\n",
    "            self.block = GASubBlock(\n",
    "                in_channels, kernel_size=21, mlp_ratio=mlp_ratio,\n",
    "                drop=drop, drop_path=drop_path, act_layer=nn.GELU)\n",
    "        else:\n",
    "            assert False and \"Invalid model_type in SimVP\"\n",
    "\n",
    "        if in_channels != out_channels:\n",
    "            self.reduction = nn.Conv2d(\n",
    "                in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.block(x)\n",
    "        return z if self.in_channels == self.out_channels else self.reduction(z)\n",
    "\n",
    "\n",
    "class MidMetaNet(nn.Module):\n",
    "    \"\"\"The hidden Translator of MetaFormer for SimVP\"\"\"\n",
    "\n",
    "    def __init__(self, channel_in, channel_hid, N2,\n",
    "                 input_resolution=None, model_type=None,\n",
    "                 mlp_ratio=4., drop=0.0, drop_path=0.1):\n",
    "        super(MidMetaNet, self).__init__()\n",
    "        assert N2 >= 2 and mlp_ratio > 1\n",
    "        self.N2 = N2\n",
    "        dpr = [  # stochastic depth decay rule\n",
    "            x.item() for x in torch.linspace(1e-2, drop_path, self.N2)]\n",
    "\n",
    "        # downsample\n",
    "        enc_layers = [MetaBlock(\n",
    "            channel_in, channel_hid, input_resolution, model_type,\n",
    "            mlp_ratio, drop, drop_path=dpr[0], layer_i=0)]\n",
    "        # middle layers\n",
    "        for i in range(1, N2-1):\n",
    "            enc_layers.append(MetaBlock(\n",
    "                channel_hid, channel_hid, input_resolution, model_type,\n",
    "                mlp_ratio, drop, drop_path=dpr[i], layer_i=i))\n",
    "        # upsample\n",
    "        enc_layers.append(MetaBlock(\n",
    "            channel_hid, channel_in, input_resolution, model_type,\n",
    "            mlp_ratio, drop, drop_path=drop_path, layer_i=N2-1))\n",
    "        self.enc = nn.Sequential(*enc_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C, H, W = x.shape\n",
    "        x = x.reshape(B, T*C, H, W)\n",
    "\n",
    "        z = x\n",
    "        for i in range(self.N2):\n",
    "            z = self.enc[i](z)\n",
    "\n",
    "        y = z.reshape(B, T, C, H, W)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =  SimVP_Model(in_shape=(11,1,160,240)).to(device)\n",
    "optim = Adam(model.parameters(), lr=1e-3,weight_decay=0.001)\n",
    "dataset_path = '/scratch/py2050/Dataset_Student/' # modify to suit your own need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract all the masks and put into one .npy file for future use\n",
    "# import matplotlib.pyplot as plt \n",
    "# import numpy as np\n",
    "# import glob\n",
    "# import os, sys\n",
    "\n",
    "# fpath =\"/scratch/py2050/Dataset_Student/my_array.npy\"\n",
    "# npyfilespath =\"/scratch/py2050/Dataset_Student/train/**/*.npy\"   \n",
    "# npfiles= glob.glob(npyfilespath,recursive=True)\n",
    "# npfiles.sort()\n",
    "# all_arrays = []\n",
    "# for i, npfile in enumerate(npfiles):\n",
    "#     all_arrays.append(np.load(os.path.join(npfile)))\n",
    "# np.save(fpath, np.stack(all_arrays))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 22, 160, 240)\n"
     ]
    }
   ],
   "source": [
    "def collate(batch):\n",
    "    batch=torch.tensor(np.array(batch))\n",
    "    batch = batch.to(device)\n",
    "\n",
    "    return batch[:,:,0:11], batch[:,:,11:]\n",
    "\n",
    "def load_dataset(train_dataset):\n",
    "    train_data= torch.tensor(train_dataset.astype(np.float32)).unsqueeze(1).numpy()\n",
    "    train_loader = DataLoader(train_data, batch_size=32, shuffle=True,drop_last=True, collate_fn=collate)\n",
    "    return train_loader\n",
    "\n",
    "\n",
    "train_dataset = np.load(\"/scratch/py2050/Dataset_Student/my_array.npy\")\n",
    "print(train_dataset.shape)\n",
    "train_loader=load_dataset(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# testpath =\"/scratch/py2050/Dataset_Student/my_test_array.npy\"\n",
    "# pngfilespath =\"/scratch/py2050/Dataset_Student/train/**/*.png\"   \n",
    "# pngfiles= glob.glob(pngfilespath,recursive=True)\n",
    "\n",
    "# pngfiles.sort()\n",
    "# all_arrays = []\n",
    "# for i, pngfile in enumerate(pngfiles):\n",
    "#     img = cv2.imread(os.path.join(pngfile))\n",
    "#     img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "#     all_arrays.append(img)\n",
    "# np.save(testpath, np.stack(all_arrays))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22000, 160, 240, 3)\n"
     ]
    }
   ],
   "source": [
    "test_dataset = np.load(\"/scratch/py2050/Dataset_Student/my_test_array.npy\")\n",
    "print(test_dataset.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader=load_dataset(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(path,lr,weight_decay):\n",
    "    model=torch.load(path)\n",
    "    optim = Adam(model.parameters(), lr=lr,weight_decay=weight_decay)\n",
    "    return model,optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/py2050/.local/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([32, 1, 11, 160, 240])) that is different to the input size (torch.Size([32, 11, 1, 160, 240])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1 Training Loss:1.1604\n",
      "\n",
      "2\n",
      "Epoch:2 Training Loss:1.0576\n",
      "\n",
      "3\n",
      "Epoch:3 Training Loss:0.9962\n",
      "\n",
      "saved\n",
      "4\n",
      "Epoch:4 Training Loss:0.9368\n",
      "\n",
      "saved\n",
      "5\n",
      "Epoch:5 Training Loss:0.8998\n",
      "\n",
      "saved\n",
      "6\n",
      "Epoch:6 Training Loss:0.8693\n",
      "\n",
      "saved\n",
      "7\n",
      "Epoch:7 Training Loss:0.8431\n",
      "\n",
      "saved\n",
      "8\n",
      "Epoch:8 Training Loss:0.8263\n",
      "\n",
      "saved\n",
      "9\n",
      "Epoch:9 Training Loss:0.7966\n",
      "\n",
      "saved\n",
      "10\n",
      "Epoch:10 Training Loss:0.7737\n",
      "\n",
      "saved\n",
      "11\n",
      "Epoch:11 Training Loss:0.7463\n",
      "\n",
      "saved\n",
      "12\n",
      "Epoch:12 Training Loss:0.7359\n",
      "\n",
      "saved\n",
      "13\n",
      "Epoch:13 Training Loss:0.7133\n",
      "\n",
      "saved\n",
      "14\n",
      "Epoch:14 Training Loss:0.7055\n",
      "\n",
      "saved\n",
      "15\n",
      "Epoch:15 Training Loss:0.6963\n",
      "\n",
      "saved\n",
      "16\n",
      "Epoch:16 Training Loss:0.6867\n",
      "\n",
      "saved\n",
      "17\n",
      "Epoch:17 Training Loss:0.6888\n",
      "\n",
      "18\n",
      "Epoch:18 Training Loss:0.6778\n",
      "\n",
      "saved\n",
      "19\n",
      "Epoch:19 Training Loss:0.6704\n",
      "\n",
      "saved\n",
      "20\n",
      "Epoch:20 Training Loss:0.6635\n",
      "\n",
      "saved\n",
      "21\n",
      "Epoch:21 Training Loss:0.6535\n",
      "\n",
      "saved\n",
      "22\n",
      "Epoch:22 Training Loss:0.6483\n",
      "\n",
      "saved\n",
      "23\n",
      "Epoch:23 Training Loss:0.6502\n",
      "\n",
      "24\n",
      "Epoch:24 Training Loss:0.6413\n",
      "\n",
      "saved\n",
      "25\n",
      "Epoch:25 Training Loss:0.6421\n",
      "\n",
      "26\n",
      "Epoch:26 Training Loss:0.6400\n",
      "\n",
      "saved\n",
      "27\n",
      "Epoch:27 Training Loss:0.6362\n",
      "\n",
      "saved\n",
      "28\n",
      "Epoch:28 Training Loss:0.6302\n",
      "\n",
      "saved\n",
      "29\n",
      "Epoch:29 Training Loss:0.6331\n",
      "\n",
      "30\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-94063b09fcc8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# model,optim=load_model('/home/yz10727/data/best_model_Simvp_gsta.pth',lr=0.001,weight_decay=0.0002)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "num_epochs = 100\n",
    "best_val_acc=1\n",
    "\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    print(epoch)\n",
    "    train_loss = 0                                                 \n",
    "    model.train()\n",
    "    \n",
    "    for batch_num, (input, target) in enumerate(train_loader): \n",
    "        input = torch.permute(input, (0,2,1,3,4))\n",
    "\n",
    "        output = model(input) \n",
    "        loss = criterion(output, target)   \n",
    "\n",
    "        loss.backward()   \n",
    "        optim.step()                                               \n",
    "        optim.zero_grad()                                           \n",
    "        train_loss += loss.item()                                 \n",
    "    train_loss /= len(train_loader.dataset)                       \n",
    "                                                                    \n",
    "    print(\"Epoch:{} Training Loss:{:.4f}\\n\".format(epoch, train_loss))\n",
    "    if best_val_acc>train_loss:\n",
    "        best_val_acc = train_loss\n",
    "        torch.save( model, \"/scratch/py2050/best_simVP_gSTA.pth\")  \n",
    "        print('saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/py2050/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:518: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/home/py2050/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVQAAAD3CAYAAABCbaxBAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAA58ElEQVR4nO2da6w0XVbX/2vvXdXd5/I8z9wQZ965qFx0ECNqIKMBibdRlDjBEBOvE8SAEo0mqIF4+SAmaLwmBDAkxIQPgBowRkNIiEIgQAyoHxQYwcAwM4zDvO87z/V012Xv5Ye9d3V1dVV39enq7uo+65d0Tp/u6rp11b/XXntdiJkhCIIg7I869Q4IgiBcCiKogiAIAyGCKgiCMBAiqIIgCAMhgioIgjAQIqiCIAgDIYIq7A0R/Rsi+pbw/EuJ6CNH2i4T0eccY1uC0AcR1AcCEf0KEc2J6CURfSqI4M3Q22HmH2fmz++xPx8mop8Yevu19f8oEX3todYvCG2IoD4svpKZbwD8LgC/B8DfbS5AROboeyUIF4II6gOEmT8B4IcA/HagGjp/AxH9IoBfDK/9CSL6n0T0lIh+koh+R/w8EX0REf13InpBRN8PYFp778uJ6OO1/99NRD9ARJ8mojeI6NuI6LcB+E4AHwgW89Ow7ISI/ikR/Wqwor+TiGa1df0tIvokEf0aEX1N3+ON+0REf5uIfj2s40NE9BVE9H+I6E0i+uba8l9MRD8Vjv2TYZ/T2vt/hIg+QkTPiOjbiejH6tYwEX0NEf08EX2GiH6YiN7bd1+F80YE9QFCRO8G8BUA/kft5Q8B+BIA7yeiLwLw3QC+DsDbAPxrAP8xCF4K4D8A+B4AbwXw7wD8qY7taAD/CcBHAbwPwLsAfB8z/zyArwfwU8x8w8xPwke+FcDnAfidAD4nLP/3w7r+KIBvBPCHAXwugD+042F/Nrzwx3V+F4A/B+B3A/hSAH+PiH5TWNYC+JsA3g7gAwD+IIC/Gvbj7QD+PYBvCufmIwB+b+2Y/ySAbwbwVQDeAeDHAXzvjvsqnCvMLI8H8ADwKwBeAngKL3DfDmAW3mMAf6C27HcA+IeNz38EwO8H8GUAfg0A1d77SQDfEp5/OYCPh+cfAPBpAKZlfz4M4Cdq/xOAVwB+S+21DwD45fD8uwF8a+29zwv7/Tkdx/ujAL62tk9zADr8fxs++yW15X8WwIc61vU3APxgeP4X4H8I6vv9sdq2fgjAX6q9rwDcAXjvqa8BeRz+If6yh8WHmPlHOt77WO35ewH8RSL6a7XXUgDvhBeiT3BQi8BHO9b5bgAfZeayx769A8AVgJ8lovgaAdDh+TvhRW/bNrt4g5lteD4Pfz9Ve38O4AYAiOjzAPxzeD/zFQBT2/Y7UTtXzMx1Fwf8uftXRPTPaq8RvGW86z4LZ4YM+YVIXSA/BuAfMfOT2uOKmb8XwCcBvItqqgfgPR3r/BiA93RMdDXLnL0OL2pfUNvmY/aTaAjbfXePbQ7BdwD4BQCfy8yP4Ifw8Xg/CeC1uGA4D6/VPvsxAF/XOHczZv7JA+6vMBJEUIU2vgvA1xPRl5Dnmoj+OBHdAvgpACWAv05ECRF9FYAv7ljPf4MXoG8N65gS0e8L730KwGtxsoeZXdjuvyCizwIAInoXEX0wLP9vAXyYiN5PRFcA/sEBjjtyC+A5gJdE9FsB/JXae/8ZwBeGSS0D4Bvg/bOR7wTwTUT0BeEYHhPRVx9wX4URIYIqrMHMPwPgLwP4NgCfAfBL8D5PMHMOP+HyYQBvAvjTAH6gYz0WwFfCTzD9KoCPh+UB4L8A+N8A/h8RvR5e+zthWz9NRM8B/AiAzw/r+iEA/zJ87pfC30PxjQD+DIAX8CL//bVjeh3AVwP4JwDeAPB+AD8DIAvv/yCAfwzg+8Ix/C8Af+yA+yqMCFp1hQmCsAtEpOB/KP4sM//XU++PcFrEQhWEHSGiDxLREyKaYOlf/ekT75YwAkRQBWF3PgDg/8JPpH0lfPTEfPNHhIeADPkFQRAGQixUQRCEgdgY2P/Bqz8v5qsgCEKDH777Hmp7XSxUQRCEgRBBFQRBGAgRVEEQhIEQQRUEQRgIEVRBEISBEEEVBEEYCBFUQRCEgRBBFQRBGAgRVEEQhIEQQRUEQRgIEVRBEISBEEEVBEEYCBFUQRCEgRBBFQRBGAgRVEEQhIG4TEFNEv8AQGkKaH3iHRIE4SGwscD0WaEUQL7mK2n/O8FOA1qDAKxUymYGnDv6LgqCcNlcjqBqDUqTlZcoWqbKgJLloXJeiKAKgjA45y2oSoEmk50/RsYAWoEX2QF2ShCEh8r5+1AVLR+7fIaU96+q8z8FgiCMg/NVk5rP9H6fJyAxfh0iqoIgDMD5KonWoOnuw/0mNJ14F4AgCMKenK+gCoIgjAwRVEEQhIEQQY2IH1UQhD0RFQGAxNwr/EoQBKGOCKogCMJAiKAKgiAMhAiqIAjCQFy8oNJ0CiipNiUIwuG5eEEFADIalCSgJNm83HQis/2CINyby04RipZpXSStA9j5En4ryxIADVB5tN0TBOGyuFxzTK2X8wPgX5OC04IgHICLtFAp2SyaZAygFDjPj7hXgiBcOhcpqPtAiQErBRTF8CvXeln0egtcllIEWxDOjPMUVKVA+5TuixCt+1Jjy5Sh9VQpL6ZJz1Pe3C/gcgW27uMe+hibk4ynOId9Jzov9ft9QJyloNJksltB6TZCtX/OsnbxGphd99n7f5c+4Itt2xK7LigCrB28i0LzvPN8cdzzqBRoNt26mHSPuAzOUlA3Mboq/HXB2IOLbNuidfi+wrkhBZrNwPP5Xqtd6XTbOO9VzQZ2Bz2XNJ0A1P86pDRd+Z/L8jBupyOxy/HzfF4d/7nPa1ycoI5KTKPPtEtMiZY3frklXEsR4JRvj23t+VurSeK70zbODW87D10otSqiXec8vn6ocxn3g9aPbfPnGsKvFbg2QoFzfl/HSjPGe4fjr35USa2vp8nIf2TOT1CPJZhK7X2jbfWZal11C2DHgNtywygCpQl44QbZv5OhlBfT5gQdu91vmHg9tHS93fy5A5zL0JZnp/3oojmBWZTgsfnVa/eij5y55yisdo9sPHeOwfUflRFe/+clqD39UXujNWii9h56bqLpmqj7THmx2PzZ6cTfYOc4PBryOxzAnTLkuSRjek861r//bd83AF9isrluxwe9Rrdx9JhuRctrZ6TX/3kJ6gOh8qcxg7ssttBT66J8qrugtReYfScnB2Kbz5CSZLWp5MqPacN/2lcoFK31VTv49RDPO7CTj/gg+xGOnYtyNO6Q8xFUrYcf7lt7lBn+nYnH6dwyfbaZLnuOPlXVMsxvkiSbh/3Bx0Z1//Mp2eQzJVqKzqZ9bV7XxgAufNfb3EDN9W7zQTaJQtTzXI7nvIdUcQDk2MeOAyf3sZ6NoO4Uw9kH57qtv7GgFCgNw8KyXJ+4qvsBz4CtQ2KtQaS8n6ztB0KpYfyTa+vdw8rd5Lut+chXaIuhrv1YVp9xDpzvZnnten7itXOQ83osEoN4RjuvnSNxNoLaF14sNqaeHivudGg2pcueg0+VZrN+wqXo+O1otL5XuBZNJ53XWWf4HhF4mq6/nLXEGSvly0+i4wd1AIZoxS4suThBBcLFZzt+pfqIqbXeL3MgKEnu575QCpQk47es69zHqryPxUi0Yg0y80EEqPd+NL/fxLRbphGjEYewKMq165TUMqSIy/X3hXFwHoK6KZazDWaA93RSH8rJva8PSuv2EoQqrHckzvkVhvC51f2RbajV80rOgbv8z11o7ZfrM2TcdEz194j8QynwhmuYdW2SyqnwHde/39qxMy99rMB2P+tDIp77Ew37z0JQV7JpjoEb968/pcn6EDD6H08YRtNJPJ/7+irb/JFdbPM/ry1P/aMmlNppqMyTHf2aiQGhBMp2oWyeB84hohqgNAnn4wIFdSUFsAFn2ShnpnmRjdPKO1ecq0Se0rT/xKIxOwsoEgPkxfpwechyjUnSuV9rvvukZegPAJN1HyqK1epinJhlYH/LMa1st1H3oRXnwHnuXUancocciVOmaR/WQt2QAkjG+Bm5TeKl1G431aXQJjrWrdxwpBTYmNPfGDuUJIzXQlsrmqZfcKWaWB8RJgIrApk4zOdVC28I/3NHumz7smZ9mK8UoDvcFkYDllbuB67Ol/HHso8BEo4fWoOc6x1exdadn/UbQwpPwOHUatukSwh14E0+K6JhQ6W2cWrLNPjb2KwLFAHBNxyHz8qL6ikFVamlZdj7M7p91MK8PP8UYjuN9u6CKEw9SjbGc0cu+Bnr15bW/gdokz9VqXY/avxx31SXobH8ipjWfKmdn9chBbaxf6wVqP5a/TrYharWwQ5FWwAw9xRymSg7nKD2SgkMw5pR+P0cnz7riKjT38ZG+5nebDyhUTunHm4YxpMxPqAdACapPw9EfrgLLAV2G+H8sCJgkoAW+U43OqUJYBvDxR7psltDvbbN8gP+/TRp/Y7ZLI+fss0ugEHZpSh6XpyfNTswwwtqsyTbNhT5GMULo/OYmH2s7NjKDO5CzKFvsJZe2fK59Q8FEamxNhue+vUyEXgSRMUyqGi5eSfpqj8yDZlkYfhPk8n2SapQRvBkNI+hSWL8e12hfV2+2zY2bWdHKE0Aty6+Y46PHprDWKi7zuaeIh/b8WGGy3W/70jyzAelXpJQNcLZjOlnRTZYE1CtV4SZjVoKRHidNVDFbQIAM6gMwhD9lM55v+quRtMu31szDI5o/Rw0jqf3ejtgRSBscLUEf3KvzRgNcIf4lvdIzW4T8irzq0dFtTPnAc74wH+x9ykVt4laGbl9/L4+L3n9ZiB3T7/ZfemwJilZDs0pWI7VzZ+YVv8vgN323WgwkfcbEoG1BnQsQh3a1hCBTe08WQaI/WdCGT3kR4giocbEadMHrtS9fmS2wfEHbd/1dE2SAaBmcsw9r7/q/FgLFkG9PAavhj5USTrngCwHTdJ1Ua0NWw9OrZJPncpNEdInGViZlHKpWQpf87OLcnViZRtGwRH5PufxnieCSzVU3mI5aQJrA+y6nUPS1+/bhtHe0s5PlxXX9Oe3pscKKwwqqJviTk+CY++/2fUGq5coOwV5sdx+aY8W6VCJaDMjSWnQ9Qw8iZXVCZwacKLBOvg2UwVuGaYSM6hwQKJr//f8Yajdu2yWs+YuWe6f2rF4yJCs+Yx38V0emvtGyGwKz0rM8l66j9BrXYXDcXHEibUjMuyduqn1xKnYNRQq+ghP+cNQD9txIf60b/bWruX86mUR6yXolF7GdGoNvprCTRJvLRLBzhK4VMEZL7DOdLkpAFU4kPP/EzNIkX+viePqdVYEUgA7BdaNsKkhOt4ORUNA+/ouOVjyZDd8rzHMahersO5rvs95qrsAGtv1xxa+u3htNEPT+u5fTJM+pKieoLTlcII6ll/myK7po2H/KTHjsLLrw/uu2dy1WMkdyvltOl6lvdthmlY3tbuewE508GkSihsDl3ohdR1XkSqjYCjojKFKP3FERkGVzvs9a5BjULA4WSswAYq1t4Sjlo7JqNkmWM1+WfWJtirnPEQfdAlLR/ZXJy1WadvIIbK2XVWbAOzYrvffBj+6dUARltklPM34MLKDFfo5UWnLYQR1oM6eg7JLV8t6OuGYjmEDe5Vz6zhemk79xFJiwGkCNw0XvVHI3jqBnSg4AzhDsCnBJYAzWJ0cqkElQ5UEVQA69YKqSgWdOdiJAjlvvQKAyh1AgJvULsnaDRqH/DpbHXG4iQEVtt3ibWGo8o2xrF4nbemlmsDJ6o8X63C8ebnZWt3GpgSLlu1WbNpuDGdj7hzis1aA9sdKRXf9gfb98qPBXi1g7smxS1sOaKGehxB1cu77XyNanZzn7THB9aF9DLZPDNzNFXii4VIDOzOwEwXWBDshLJ4ouNSLKYdIG5f48KW6hUoMUNB5VRB0DqiC4Qx5cS0ZziioElAlg2v+2qYokmOg7iMlwKUKVLjKUt1FTA9KuiUGdxtEADZYqZt8m1p3prRGP3cX/v3abli7LrDRH7stjCqGhx2w9OXYeZCz/CvUHOVng7WbXRpaA7DhRmspfRhK4ZFWPispMeDpBPbxFHaiYScK5bVCOfVDejsBsid+aM86PBRXwrpyQzo/1IcDdE5gA6icoBXAJYMLHyNpMgZoOYHvWHl3QOOwWIWYSlr+TzF0Cusi7Bfiqh4u2R38z33oKiNYj/1scX+x3uLTDK6UVmtxkz801gegFveC6rfdejgvWIHjbKCruQSUAlQtdK9F3Kv42J39vjuWWRwxwwmq47O08kbjM92BXgWGN6UMau0LhUxTIE3gZgnsVYrikUFxpVBOFcoroLgJw/oEyB87uJSXwpZ0bJ8BKggq959V1gsqa2+xqgRQOQfhJDAt/aJECroWO8rkh6ouUdV2e1G35lqKNe9FWxnBumC1zK7H49gYrG8UYAlkd7Tuuob5CuB099ubjUL8maNFY1/qIWAdKdD3SfmtCpCHiljnzDCCGkq0bWoJIQToBC0+mtudpODZBG5qYG9SFNcG+SON/IZQ3BLKGVBeMcprB04YnDgkjzNMUwvaMCtUlhq2VP6RayBTULmCSsL2C2+9JiAUAGwKqJKgcwarYKjUXQCWoetucN4QJhVSUQ8Zg9qaLhx8l12z+1W67L6joHpc6pbwLE50EMaO95UXeJXtMTTflh5bTyceUf2JQzPokJ+L0g/Bdol/65ECurHKz32JKaKnbIV7D3iXGd9IDIGKE06TFO4mRXmVIH9skN8o5LeE8gbIbxl2CrgrB1yXIO1gjMNnPXkJo1ynoOZWY1EYFFajKAxKzSjJ+CpqrLy1CoKDt3j9tD2DLKoZfACrlmhrXGs8EctUUyp9aBkxVxYpOV4Ly7nXuYvbbWtbY/RqValGeT4/kbY+HO9EeTFE6dp/GKL1u01MGz5TVrSaERXedo2JKtWID+ZEL10rbbHDLSUH69sEsNSDPmmsscQgMGzZwFBf5BjW77A+VGvBACheYM3WujHls8m20AmtfH3DoUV1qID5bTdMSJXce+jp7nGRUagRmiZeTGcp3DSBnRkUjzSyRwrFDaG4BYobRnHL4JmFuioxneW+dokp8drt0+VuMMHx6jG/LCZIlENmNTLlsKAEzATLABcEp/3xKyZfP8N4fyt8aGPwlSK4ANr8oo3XueY/dW7pe4wWU7Me6q7nbmUY39L+OoQXrQiVVqtiR9SZOda1TTa0rEnQxpbwxFafaVd8+Lb7SRMY/nujtlt0Q8nBan9CiixFv3YPV5X/IK8kduxFreX0oRl+UspaXzgaMfNmeSD3TfnkRRaGWyOcQ1Mb2ggDq5XSnTt6zVWaTCrLFGkCezOBnRmUVxqLxxr5Y0JxDRSPGMVbLPRtgeksx800wywpoIgxMwV+89Xr1TotFF6VE7iaOTnVJe7KBAubYG6CmFoFlxA4UeCEw/IMlwTrhQmuZIAJTFzFS+ps/aYjx1WI1cHZ5pbZUGbxwbGh5GAdTgyI7MVHABxUoTgvANQEdI+MhXrc5d4xrxvaWPSF0hBn2DOhoT7cqOLudmjzsXPcaRSFNAGnCXhiYG8mKG4TFDca+Y1C9lZC/ggorxnlrUXyOMMsiOlbpnPcJN6Bea1zvGfyBl64KVyoTPRY+xq2BWvcOX8uFDkY5aCIMU8SlE6BHaFIFFxKUI4Ai2qyihVgU4IqNlstTb9pFSrFDIpxqc4tS/QVG7reNk9TW8nBTd+p6ciia4RNxRjeU+PS/v7bzpheIrhpeK8rZnWbT3VH6p1jB0lTDWVCOc8PatQc1uQbMuVrqHXt0saijViurY+QWut9QZveB7pF1Tlv1YZle6O0P8Zqf5WfLU817EyhnJGfxb8OYnpjoW4K3N7McZ0WuEkzPEnneJQsoMCY6Ry3eoGELGyYAc6ct9AW7Pe9VApWExwrLIiRaos5hQpQxnl/omGwpTDE9w+QL6GpNpUnrd1LZENolWVQ9Js6tz6crD/f9j1s+y7rJfma4VGNdFgmAsxmvykb5YXpCCFCvnoZ+ou7ovYws9i5Fe7+CQjxPPYN/h86+/IIUUgjHEP3gN2yZ0z9Aqg/74g93HuCq2/3Tee2O9Zjs7SuC+c+zdSIvJiGIGtWPtPJC2oIiZoS7JRQXjHstQNmFpNZgSezBWamwE2S4UkyxyMzhyLGlcpxrTJcK2+xWlZ4iisAQMJ+/zJnULBGqTQUGIm20Mr5EErNIW6Vq5jSSlAR4lqbge2MdcGJE1EcagKE5yjtaqm5mm8V2GOCoyaWa2UJozjU00mDz7R1hn0l7VQBzh4ljZZKBxi1Mbh//UPt/v4Y2uX3vUN0N8Ba+RjiPr7UlfUq7N0W/kicpaBWKaWhbB5nWSV0o2inAuw0ozjk7GPl+1PK+/lCOxE7NX6of00orgnFVch0Mg46dZgkJa6THFcmx7XO8cjM8djMochhSiVu1fK82lpB4gV7S/UzuAYAGGVxk2R4VaYw2iFJLMpiVYxsyqBi883nQ6YcVFb6oah1y1n9+tAzpkQ20yN7xjRyUQDWtfvB06S72EnL8pzozkkoH0tLK8uy47WZ9VPjC9Fwd3iaJrDS6zGqQK+Sg6wImKY7lQL0bdPp9A0pe3CWglrhHHi+qFki2y9OzrIq1fIgWHuaxnn16lC10BqXGrjUW6c2WQbqcxKC81MHbSwmSYkrk+Pt6Ss8Se7wG9On0HCYqgK3aoEndUENk1F3PEGCEk/0HZ6ZKyhiPC+neIWOCR0FuIShS6r9D6gtulfVPw2l/2hRLP9v9FfirEf9hvq5AgBmf13EH6Mw0dIqpi0Fo6vA/baBBtGamB4bH1bG3fn89WXtliF9V+sZAcC5CyqwWuau5/JsgyTcR1RjfGPbxET01Z0qfa5Wuo1V8PUaBZeoqioUG58O6gyDDUMZB2MsUm2hwDDKYqoKJGQxpRwpWWhyeOGmuFUL6DAs1+SQwP9w5PATUpr8hFT1fhj2K8VwmsGOQWV9iNyIQQ2QRXVTU+nCUDP4TAvrJ53iOS4K3zm3+rDansLIzvsbqopPblkM2i6vJ0ItxrTqGEqtw/xWyzSkk3aKKQV/6qYwqSbxOt+xPB8xuioF1NbN/Xy7moDyvFNED8V5RbUPxT5WpOv+LJfl6XvmxJtMKXASrFNDvtReUq8QBS+ompEai0TbSgwBVNZpEhoyvWFvVjajwZhSgWlrgKIPo0qUQ2ostLEgs0xb3QijyumvGvFxTUyzwk9qlD4Eh4vCDwXDg4zenqzBvFo2LiZ51IuM1MN7or/ULCcjOTQN3FTJyZlGnGoTos3vt1GU/nGA7g2q6BF83+Uj3oFRFLM5EOdvoZ6C0Ll0lKQJeJJ4H11qYCcabqJgUwWb+iG2SxkuDPeTtITR3rJ8lPiZfAXGE30HAHibfokrleGFm+LdpsCCGW/aBE/UHM95giLM8j/Wc2QuQUIWj5IFXhbb02vJAro23CfH0LkXU1UufaZq4QWEitLHO6aJv/GLcllGz1pwUeztj+bEgJTb7AckAk833zorZQgPQWiX0ywTqBZle+opc5VqGlNPV97r2fmAStc95O8RNkWOT5qKeuhyfg/TQgX80H+RDVuJ6FQYA5qkld+PtRfTyneahjqmCYV6pt6XqRKfm6+Vw0T7m22iSkxVgRduimuVIaGyGuYfClUAumAfOtW2qXCjsgpB5LWYU84y/xjCb12UPlog+lAnLemm2wh9rwbHbRb5FTaJHvzw37tO/KNvwgQVFtjFPdFC9R3ueF6plpa6Nx0904bgYVuo1oZ8bzrfoi5Kg0IOOYfUR1+hSYNrvtPl3xCmFIb7Wvtg/KkuYMj7TyeVD7XAlEok5JCQRQKCBSMhhyLM9Cs4pDv3aV6F2KehxlAccuyLUxerNUCroaJ1yzz9+/iru0rwOe9b9cNwWt70NrSh6SECTNhtAqrPD/qmtM2W/SJmH2WkuN23y7x7yJZl78fuOt8b0k+r/ap37j1lmT5Fy7DLgXnYgooQspQkoGbdgTNhLdwnMT7V0yg4o4LflGAT7zd1SRjuG4YJvtNZUuDK5LjRGW70Ak/0HT7bPMWtWuBWFdBgOMpwoyZQnGPBFs+CoGpyuML2mfWVeaP6vcRYs0qpcFB3BVRWeOs0TYKILq00du7+YTSkNqcL13e6akld7F9EuknfsKlNJQiLsrVgCjGD8nKYjC1mqHzLue6TUurc/VNPm37vkfLgBRWAnym2dphW0MeiXkMgFD5BYuCmBm7iq+374T6hmBHKa6p8qJwyKHVI0xKzpMS1yfHIZHhs5nii76oA/mpTAB6rAgoJpmTwRDGAAooZT90UT+319v0tFCj3dVJVTtCF95/qnKFzXx2KLKAzB/N8sVoYIy9A5TIPfKg2JlvJi+Xs/ySFS82ya2vwVe5U/GRXNrQeWaEuUg2fKmXWB/bfcyKJSrf3MH8QQiH4sddLFUGNBJ/q2RScjuE7MVtHqxBzanw30lSFmX0sY0/DzD5rribcJ6bE1BSY6RxXKg9W6Ry3aoErKnFFjGmwcDIu4RolgHw0QA5YYKIKTFSCV3Z3/1Q9VApoqTgV/t+nBB8A72/uGrpvKY1H1vqZ+USDCtvq7iXHQLA6eUMKamu8Z60k4b2J4hquDWIGd5QDbItOqLLP6vu0MQSNe0UcUFHuN1+xLX14JIig1onxpcTjHv7X4yCjoMZ4SEXBj4eqfQiH1+LrYIAdgUMZvliKT5GrgvZTOEzJISVCEvyNGZewYBTMWLDCgjUKNijYIHNJeBjMbYKFNWHdgHMt59J5v6lvsxH8pi6IqkPlN6xSSmP8755haVTv6tmkXtu07bOWfRyvpvZydnG5Wvozd2xqLd6TfQxoJagd/mFqE5V6fYF6nCp7QSfmtbhR3yKl3SfbO1c/+na3zexHd80eP4TMfPqQxB6IoDbgPD/oLOAQrBU7TsyKtaEsg0tGTFhSln37kQKggkBGgTWjKDSeLyZQxJjqAm9PXgLw/Z3elt6tbfcFO7xyCnec4Kmb4YWb4am9wqeKx3hWzvDp/BafyWd4Y3GNp3czKOV8Gb9SL/2ksexC6Zv3+aG/H+5T6WBehiFdDJOKPtN9uryeCCpd784tVAsTA7CceKsv4zpcAGmCtSDf6C7o8PtGH+tebGocGLfTtc8XighqG9aC54vjt8a2FpwXu283L4IVwNB5CbpKQU4vezVZApXw9UcTAjFgrUaGKYrcYJEnyEqDuzLFa1dPcTdJMVUF3m3eRIECV2RxqzRuSflgffaip8FIqMSUCnwUb0fmDEpWviZqkuD53RTZPAE/S2FeKuiMoBfA5CkjeQWYBcPcOejcQeWudyzkpdGaF99cJiYztFGUqxaz0ctiLk0x22dibYfyfM2U4H04ms98AERQu3Cuim2klsZrB4HUbnVamb0VB1RNErPf8AjlTMOlBJsQ8htfWcpOgfIKKG8Y9sqBryymtxlurxZ4y3SOd14/w7umT/HSTvCJ7C2+DF+a4LPNM7xV30GjxJQUNBEUMzTYp5uy/6soBOGDoYhBxGAmsPPVpWLEM8URbc0FQTamPNaOLcSaVvGlRy7MvbofWObvD+HnjIR1rfkoG5YflXbzkLn5uq2tU+tVN0Zp1wVVhRCx0i47qLaJdw8xXWk9cyYiOCQiqJsIYRqslO8gemhrVdFuXQmcA3LnK0ppBdYai7ca2Ilv/+wS37m0nAJugqpcH1+VmFzneMejl3jr9A7vmL7Ee2dv4D3p6/iF+TvxvJzhWTnDczODst6vOqXn0MSwjZvEwtdAdaxgoeDgfbIck/QJvnSfCqmnsaSg4vUW1LHgCbO3uIce5nfm1G/4XmO91eiPtMs4L2L2mtHH4mv1h7aIM/P6j0effkx1nFtGSShV/YgBWLaIXlmegISW21W08w9Y5TfuOUm1kSHrKB8ZEdQ+xK6us9loJqvqPkX73s9C8SiFSwjJnQOxTzMFE1Tuu4mygh/2FwTONDKk+HXc4FWe4EUxQeY07myK901fx7uSN/Eu/QxfmCb40UWCn1u8Bkw/jhdqjisqMa3doQUbPLVXeFbO8Kqc4EUxwasiRVYYPxnFfpuqIJD1/6vCh0rpgqHyZREUyqyPd8zy4W+qTW1NtrU06fBH+joDql8Vp6xHR9a+YVK70Fgfpcl6/YD6dq0F7qOH+0Zf1Hdn5KFRmxBB3YGqNBzRSSetuChAN9dwj6/BqQFrBZU7AArFtZ8AUiVgWzSCCgIpAiuFstTIS4NXeYpP0SMsrJ+lfzaZ4WlyjYQ+jveZO7zj+hV+7NXn47F+hSf6Dk/0HRwrPHdTPLdTvHBTvLQTvCpT3JUpFqVBVhgUCwOeG+jcT4jFh87hJ6MyDr5TB7UovZjmBSgrDt6qYvWEhjKAG9pBrxHbOVsG2QGs6JYJnmpCZ8ihc/S3DtAXa6XVzAMc3rchgroL8YJXyjvoj+EGWNm+Hw7SWx6DJylY+WwoIAyXXRxWoztchxEcmD50qiw1cq0x1wmIGG8aH6RfOIMpFXh38gauqcT7px/Hzy1eg4OCDhbqnZv4h51gblMsrEFmDfJSI88NONdewEv/UAWgM0CVDFWG9NIy+BGdW6YvxmMZ6iaNbWu66Ovvs27pY+TlsP/exLAjYC11k6xbe225H3a1ZGEL1HXMtdRP2ndoPlSpyh7Hcy6IoN6H2Ml0OlltxXLQbTLAvpYr314DRvnCFjoW41hu3+nQqrnHLllLKKxGbjW0MniWT2GZMA+N9xac4H3Jp/Fl0xw/c5eiYI2rkEn1KgqqSzG3SWWd5qWBzTQQxNSHSPlC0jrjKmRKFc4/8mVq5VppN9eICa6/3/ec921bU8P3YmqsP1rMzXYo96XNXxqpFYBZ+1i5IRU1LkPk/f6bGEMHUufOIqW0LyKoe9BsxXLQbZUliAj0vtf8zVTYqu0y4IP43SRU6Te+m6hrCU9cfoDgcg2tHaxVmOcJZkmBRfATOFZ4Zmb45ewdeGqvUPAn8NHF2zBTOSwIj/UcL+0Uz+wML8opXhYTPF3M8GI+wWKeApkGFT5UyrwiJK8AveAq3VTlDioEm1NmQXmxEncKwFsui2zpu3a8bHGTJP3y8Xchy32mXMjdp0na3wWw43YOTkeJSd+O/fRF5jgvziJQf1dEUIcglgIEvLgOfKPzIgNdXYGupqE1N7rbGSO0aC59ummEGFAFwYGhlLdimQg2UVW1kkXpL4eYOfV6doNXeoKFS3Cr5vjVV2/BlcmhiHGnJ3hpJ3heTvFGdo03F1e4yxMUhYYrFNRCQS8IekE1izT8zX3dU2+drt9UrTdbUfZqcbM3MXSoqwVKjKwI+7TWEiXUad2HTTGc+8ZktlqD97Dgd9pmW1uaCxniNxFBHYr73uyuMexTLb4vZpD2FfirC9ExUF+MUYXKxFJ4FFI+KUT6kKv5UB0DTGCr4BTAmlFajTyIqyLGi9KX6VPk8MLN8DybwjHheemt8afFFV6UE7wsJpgXCfLcwBYayFUoguKH+aqsP4LftHDLmpzWen+ec53ppdwcHsdlh667EP2awYqLJeeqmfHomgh1UxnwYUfVfg3g++3y6fYokddr3U1sS2wqsN+5Dd8VP7B4VBHUQ1Bvax3/7/IDYj1MhGazzlW3+vbgBVQVDnYWhcD7Le3EFx5RJeDStY+BS/L1URnIylr6KnHVEuUVOby0U+RWY14meFVOoInxmXyGl8UEizJBVmoUmYGbG9BCg0pvEVf+01BEWkffaemtUyrssq1HUfT3p4WJDJocoOxiCCOiSRrCiByglyevmn2vdQ4YipO0B+kojUdE93YPXJJfdBdEUIfG2srP5/1V1O0H7KDr/dg+gibpSkD8rpAFVE6+Bon2VmpZ+EuBmWDdsmjKvEzwUk8w04Uf0lsFo269VVomeFmkeLGY4MVnroCF9kP9OSF56dNM9QJI7hjpS1cN91UW2kCHNh60yDdaMSdrDR59nUfqWEq1eq9j4JzjQTcSfPOHQAT1gHAzSHoPPyBbt0wzBXxVoKLc2tuoDpUAmu5d632pjgiWvErHLKfSahhtYZ3C6/k1FvMUhXHQilEkvrDKIk+QLRJgoaHv/FDfzL2Ymjn7MKnCD/Ormf1gmfpwqTCkLsvVIX2fcxWjLY5QcpFCmNyQk1TUrNR0Ce14Ro73zx8uE0sE9ZDUBSGEPN3b11qW4Jx8+TnnYhW+NfyQsXbT8/JBruZHdeRTEi3AUN6tqhjRMcuh+Z7RGjYp8TSbocwMbOnwSqUoSo27RYoy13BzA7XwYqoXBJ15y1RnywLS3jp10Jmr/KcrKZUN3+Oaz7QLaw9fZyGmUxKBerZhay2z12RDaJRwIKLv/UCIoB6JfYcYPJ8DWQbMpl4u02TZ8jhADKic4cxSVHXBvjd8XKYkKLDPAYhDWfai6ghgdnCOEAeeWjsUpYYiBs81mDTuSoVF4uAKBeQKtNDQc4K5I+gcMHd+mB/L8yV3DipzMHMLtSiWGVHRd3ouFOXG6Iq1ZR/QZIzgEUE9Z5hBixKKGQ4GCFlTqlZM2KW01q6Zo3UKXxHKOVSiyrnyhUwSb2G5QqHMfeaTfqnBxpfXcLkClQqUeau08pnmCGFRDLNg6IWrLFOVW98SOstXQoNW4iUd+zCbcyiQUZQgF6IvIKmYggjq+eJC6iAz2IWK91UZvKXlSaWfsXcgKIJvgxLeU6WP+FHwE1RwBOhQZd4trTAGYEuHdOEnsWJeq8rDLH7uh/lmzj4bqvC1TnXmg/d14aAXpRfRukVqLfieZeJORn0oH7sKoFy+J0I6Thz3yjDbFxHUc6QqKVcrI+d8TrzStKJHvq2IF1Uogg2+VFah8HSMOwUBIRmAGrParBjsfKA+lF8fUAuLKlHN6PsqUqgsU1UyKHegeRjiuzAR5ZzvXFqPOQ3ptTvjGFvb1vRsA72VZnm6IcrVHZK+E10jqaJ2EOJ1dYRQLhHUM4QLP1ymxE/ZU2G9CKbbfXsqjrTZx6WqksDOt5cGABWs2yiavmYpARmQ3AUhLqlal86XlmkyDxlQubdO9byEyko/zA9Vk2LoV2vGD7t7+Zq3tq0pS5/b3lW+74Lp5T4ZecufvbH2aCFgIqjnTiwjV4NK9nP1Qa8oNMvzL4TntWAAVn4ZYOktiO1TuGa4mFe8Iqg+Jz8KaWhnkgXLtPD+Uiqsn4CKsabMrWLKeXG6qvz3gLNsvbfXIbG17gVbWDmXfdwnoeUPAJ+CeozuFBeKnLlzJaYLcuob2hkFsgZkfayng/I5+4mvMq9AcAbVcwDLEn81basP/Kihb2YRxDcMI1VosBf9pStiepeHnu6h2V4Ig+J6+qS1yyGp3d7wbSMxY6mrpGKIdR0sZ71PSmV0awQ2dlzdurkdUzh3PZdhebZ2eQ0cuzzlBSCCeq5EgXAMho/rVIX1NwAp3yK4hJ98soADg9g363PsRbWau9Lt96oqsCK2ZhEsVBeH/AyTMfS8NosfKu/TXVYF7ceJKHZu2dLEsZ8oGGooFoP8Z7P1VN9IWW6vjTogXK7WJWClQemBLdp9kwOsrRIqqvKU5yqq4Vwcs9aqCOo5wyEVNU3AyvnumZPVr1QXDGfIa2zMpNReVOv2KLfcM8SxgV5YV+atGJ0HQQ1Vo8wr7yulvPRWaX2IH4RubdcPFBrF83lI+W2/tLko/Mx8R0UwzguAHWgyWSt/1yx9x0Wx20SHs+DFuluDpptLP+5SYWrI88qL7DBlEo9BjxTvQyCCemFU5fAYABSYeJktBcAmVBVOac+1WuJbTy8Xi/2fWAOq8O2fKaaS5iUo8+mjlBXgLF+fsa+nk54yNMpZcNax/Rgb21Jy7lAFP1rL260scMJQLGvBC3fZk1YDIoJ6bjT9hNb6mFHnQNYChQJp3zdKFa6We07LDqTkRTW2cV4jhFapMnYi9S9HC5WVf65i+b3qEfLxi1o901p5wr1Sb3eAmf252Nb2ZPNKdv/MfRlz7GoMN2vLaBurj7Xumz8yIqjnRLP1iiLfsI+oyu0npYBU+0D/nENVf+WFkZT/n9i3leqookTOuwoAn3UVm5yau9hWm7xVuih9LdMsWG71EnxVW+H2If9BKQqwVb60X2SMN/650OG2oTRFVZR3ROeX7XFiTtsQQT1Dtvm2qHDVTLydaG+pRgODVWWV6pzhkuXklM6Xv+qxLkC9CZ1alECtjzxluc/WCiXnqkr7Y0gfDa2/AWz0qQr3py6yY2qxfkrkKjtXrAXnoc96UfgK/QBYKSgAnGiwUVCFA1SwRhV7n2rtulfl8v8qTIqjZRrSWYM4U16uZAXF4iZVWxbnh/QcMqLGQkw5XPkB2hRh0GxjE5aN9W1HyYnPe+9RyAFaBK3ux2njmUVQz5XqxvFV49k6UGlBqvTtOhwBlvysO4V0UvLD9fqMfjXqr8eiOj97D/gkgaoUXVGuth4uSi9W0V8afVdjC9CP58o2Qpa69pN5ddl4TNa2h0NE6MQhRqc87323be1alTQAw527feOZ90QE9dIImVOkFMiWgIJXTYqZULRiodpUhQZ+taG887VLAV/Xk8Iwn/JixUJthvNwnJQaI26HtNaOZbdZYeJa6EfbuR3k3I2gQLd8+5dCiHGk6dRX8g+/0ivD/ywE2NeH/FlLXKTjKvzKZzuF2f1wI5BSS3GJViowqmH+KeA892FjEmK0M5znVdIHTSY7Was7p9oeEBHUC4PzMOvv/LCKlQI5ByqUF1VmQKmVotN1qHSAZf+ZMrToiL/8oWoPx/9jXOkDF9IVmP13YMzxhv/HarF9aOrXkbXejbXJ3+pqrXNGcg2KoJ478UaKPihn/SSV9XnjpEqgJLAiECegwoKNApL2GE3KLChcnLRYHeIyhzx8W3phLcc1+TQKXPgR0upoaZvHiu89Gux86JO1637vJiPrriqCes7UfX31Emz1cnVhBp5iD3kAZLUvRN1GfaY4L9AVdH6orpGXAi8y8anek/q1dW7XmXzbl0ook1fVAGVeWpxGg137V095sd7GwzlwUfgSb2PO6hkZcTh6MJ/qGOJ9hRVEUC+FWlWnKiUwtmeuQcastu1oEoZacVKLYjEQZpDWlze8PCT1tM1DpWmKmI4KEdRLoZYeuJIS2CxKHNpQd+kpAC/O4XNszFJUta7qmgo9aUkXFi4XEdQLZFNK4M559dEfO53CPX0mYnpPzroUntAbEdRzJqTx+Rz69qEfZ1kV1N/HlxfTSKsb3xg/oZX5SRYuy9HNrJ4NA5XC2/R9C6dFBPVM4TD0Jj31hUu7iDdeVwm2OolZ7XiqtJ/44FAA5UiV7i+W+F309al2pfGOKO5SWEUE9VyJrSrcVb/lO0qwVSgF0nopmo5BqQbP58sMKckAGoQVn+rGBU9Q+lDYCxHUM4WMARmz1uaBZrOVySnOMj+z38e6VOTFMxaEjiE5SvmWIBKiMxjnFl8p9EME9cLgPF+28chzb5mW/YuWkNZgpULVfecnUoiqdQmC0I0I6qXRJpy7CGFiQCBwc95JZvcFYSsiqMIqzRbMRbGllZ8gCJHTCmqSgLQSf9KIkEkQQbg/pxVUa5fWT5L4oakMLQVBOFNOP+S3NoTsKEArH18JyASIIAhnx2kFVeu1VDya+fCeeviOIAjCOXB6C7UDX+DD04y1FARBGCNbUjVOiKLlbLMa724KgiBETq9U2zoVSrkzQRDOhNMKalH4dMYt0GzqowAEQRBGzCjiUPtAxgAxBVIQBGGEnNZCjS2Ki7Lf0F+G/4IgjJjTB/aH0KiVcmYinIIgnCGnn5SqY20oESfZ44IgnB+jiUPlovRl50KG1ForjqKsrFlBEIQxMhpBXcmKYueFlRmw3oiW9sWCIIyd8QhqjXr1KalEJQjCuTAuH6ogCMIZI4IqCIIwECKogiAIAyGCKgiCMBAiqIIgCAMhgioIgjAQIqiCIAgDIYIqCIIwECKogiAIAyGCKpwOpQCtT70XgjAYIqjC6dDaN2OUnmHChTDKXH7hAaEINJn45+ykdoNw1ohpIJwWx76tDbtT74kg7I1YqMLpsRYsw37hAhBBFcZBUZx6DwRhb0RQhdNRFGARUuGCkHGWIAjCQIigCoIgDIQIqiAIwkA8HB+qUst4R0BiHgVBGJyHI6gAoGj53IlxLgjCsDwsQXV86j0QBOGCeTiC6hx4PvfPkwRkHs6hC4JwHB6mqlgLtvbUeyEIwoVxnoKqNcAMuHvmf9/3c4IgCBs4y5kZSlM/ZFdKSr8JgjAaztNCBYDEgEJxYs4ysToFQTg55yOoSQLSDWu0HgYlCIJwYs5GUInocttl7OsTFgRhFIgD8tQoBZpOABJrWxDOnbOxUC8SiYcVhIti3HezUkvBubThfvQJix9YEC6GcQsqEZD02MUotmfkg6Q4xI8JBixpsYJw7oxbUHtCaQIU5Ju9nQnntK+CIPRjlJNSNJtd3hBfEISLZzwWapIACENhRVXQfm8U+XVIjyJBEE7EaQW1ljZKWq1apX18p3W0BpGSpm+CIJyM0wqq1t7/KQiCcAGM0od6bxT5IHnxvwqCcALG40NtYm13hX2tAXb+/aZrQGuQY3BchyAIwpEYraByUXYKIk0nYOsAa0FtvtbEgBSBY2znGcWnCoJwvoxWUDdRdSvdVAtVa9AslPfLC5n9FwTh4JyloO4KGQOE0n+cF2KxCoJwEE47KeXc/n7OTb7WiAql/2SyShCEA3JaQbU2WIz3zGN3zg//WSxOQRBOz+nDpmJ75yPNyNNkUmVlCYIgDMnpBfXYKAJpBUrTU++JIAgXxoOYlFpDa4CkXJ4gCMPy8CxUQRCEAzEaC5XzAtCuyu2nNA2TVtvrhjY/2wtFoNlsNC2oKU3XoxDqxx8q/FcxuIIgjI7RCCqcWxUURQD3bA8SBJGD9pAx/VqLjKn9iKL1/akfv3NgeOHlshzFj4AgCKuMR1ABLxKO7yd0zi1FhghAEOdt6yLyGVfHFqhNWV5ty8U209OJj4iIr4uwCsJoGJegWguez33F/j2sx7qbYNu6aDoBivLoLUkoTbYnGtTSZ1c+O534J459yJkgCKNg3JNSpJbicU84y8DzxXh8jyocE+156sOPAE0n/a1dQRAOyrgs1CaKAKf2G5LHz20SnWO0T6knEwyRAqsIYN/lgAyDrZVyhYJwYsYtqENTT3GtuwFi+5QoSAfwS/aeKOuL1su+W/VyheJTFYST8XAENaa4Aj4EqRlipQg0m/rn1o7HRdAXrUETJT5VQTgh4xdURaA02Vhwehda4z237oPy+9BW+k/r9iLXpyC0gBnqXAmCsBsjUYKACp1Pm0PjIduatK0f8O6AsO6q0n8U3rhf2oWQrCU0trKA0gJGEE7GuASVyPsa24h+wgOKxErolAoFVGriezYdWo9wrgRBWGdcgmotOMt8ib2hs5iU6rXelZCmMWVS7YrWPgYX8JlV0gJGEA7OuAQVOOwsdZdAWuv9jtuWOzcUeb+vWKqCcBTGJ6hAaBGtDiNsbeLiGj7TS8IFv288NhFXQTgYoxRUXmQhQ2p4gWudqU+SvTOyxkrzuHi+kFhVQTgQoxTUNgYfukaf6gODJhPxqQrCgRitoPpYypolZe1wllXMMroUX2kfHIPLEmSMr6uKA6faCsIDZLSCikPmpisFjCUY/1iw8wKqlS86owG2JyhbKAgXzANTFaFKqVUKNJv6/2WiShAG4eEIqnPLCZlLnM3vgBeZL04tCMLBeTiCClze8LaWLttJ1/vOAUUpYisIA/KwBLWOO7GQNCfE4v4o6r9v7PbqNHDsLgWCcOk8TEEtCvCJZ7ibrVmiuFGaSgk+QThTHqagjgDOstXKVaHxnliNgnC+iKCeijZ/7qX5eAXhgSHd3QRBEAaCWGZ5BUEQBkEsVEEQhIEQQRUEQRgIEVRBEISBEEEVBEEYCBFUQRCEgRBBFQRBGIj/DyN1ANAbXQS5AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "model_path = '/scratch/py2050/best_simVP_gSTA.pth'\n",
    "def results_display(data,model_path,num):\n",
    "    model=torch.load(model_path).to(device)\n",
    "    data_test= torch.tensor(data.astype(np.float32)).unsqueeze(1)\n",
    "    data_test= torch.permute(data_test, (0,2,1,3,4))\n",
    "    data_test=torch.tensor(np.array(data_test)).to(device)\n",
    "    output = model(data_test[num,0:11].unsqueeze(0))\n",
    "    plt.imshow(np.rint(output.squeeze(0)[10].squeeze(0).cpu().detach().numpy()))\n",
    "    plt.axis('off')  \n",
    "    plt.title('Predicted Image')\n",
    "    plt.show()\n",
    "    plt.imshow(data[num,21])\n",
    "    plt.title('Label Image')\n",
    "    plt.show()\n",
    "\n",
    "            \n",
    "model_path=\"/scratch/py2050/best_simVP_gSTA.pth\"\n",
    "data=np.load(\"/scratch/py2050/Dataset_Student/my_array.npy\")\n",
    "num=1\n",
    "results_display(data[1:],model_path,num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Inference ...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-261c28de118e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#     x = x.to(device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'out'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mmasks_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-4ae6c3d273ee>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x_raw, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_raw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_raw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_raw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "\n",
    "model_path = \"/scratch/py2050/best_simVP_gSTA.pth\"\n",
    "model=torch.load(model_path).to(device)\n",
    "print('\\nRunning Inference ...')\n",
    "model.eval()\n",
    "masks_test = np.zeros((0, 160, 240))\n",
    "\n",
    "for i, x in enumerate(test_loader):\n",
    "#     x = x.to(device)\n",
    "    with torch.no_grad():\n",
    "        y = model(x)\n",
    "        out = y['out']\n",
    "    masks_batch = out.cpu().detach().numpy().argmax(1)\n",
    "    masks_test = np.concatenate([masks_test, masks_batch], axis=0)\n",
    "    if (i+1) % 100 == 0:\n",
    "        print(f'Batch {i+1} / {len(test_loader)}')\n",
    "\n",
    "assert masks_test.shape == (2000, 160, 240)\n",
    "\n",
    "    # save\n",
    "np.save(exp_dir + 'masks_hidden.npy', masks_test)\n",
    "print(f'\\nMasks saved. Shape: {masks_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
